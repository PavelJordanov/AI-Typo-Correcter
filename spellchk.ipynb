{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The approach to solving the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the default solution.\n",
    "2. Compare the output to the expected output.\n",
    "3. Iteratively optimise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running the default solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n"
     ]
    }
   ],
   "source": [
    "with StringIO(\"4\\tit will put your maind into non-stop learning.\") as f:\n",
    "    for (locations, spellchk_sent) in spellchk(f):\n",
    "        print(\"{locs}\\t{sent}\".format(\n",
    "            locs=\",\".join([str(i) for i in locations]),\n",
    "            sent=\" \".join(spellchk_sent)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So far, looks like it does a good job\n",
    "## Let's write functions to see sentences compare to expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n",
      "8\tThere was no doubt that Herr Schaffner meant every word of what he said .\n",
      "5,14\tJust before Myra left -- cathy was saying good-by to Cathy , and she did realize I was near '' .\n"
     ]
    }
   ],
   "source": [
    "def print_corrected_sentence(sentence):\n",
    "    with StringIO(sentence) as f:\n",
    "        for (locations, spellchk_sent) in spellchk(f):\n",
    "            print(\"{locs}\\t{sent}\".format(\n",
    "                locs=\",\".join([str(i) for i in locations]),\n",
    "                sent=\" \".join(spellchk_sent)\n",
    "            ))\n",
    "\n",
    "print_corrected_sentence(\"4\\tit will put your maind into non-stop learning.\")\n",
    "print_corrected_sentence(\"8\\tThere was no doubt that Herr Schaffner meant evey word of what he said .\")\n",
    "print_corrected_sentence(\"5,14\\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code seems to work in some instances, however it is unexpectedly changing a few words that it should not be and thus needs to be further optomized to ensure a better success rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction_v2(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    predictions = [item['token_str'] for item in predict]\n",
    "    most_similar = min(predictions, key=lambda x: Levenshtein.distance(x, typo))\n",
    "    return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellchk_v2(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=20\n",
    "            )\n",
    "            logging.info(predict)\n",
    "            spellchk_sent[i] = select_correction_v2(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corrected_sentence_v2(sentence):\n",
    "    with StringIO(sentence) as f:\n",
    "        for (locations, spellchk_sent) in spellchk_v2(f):\n",
    "            print(\"{locs}\\t{sent}\".format(\n",
    "                locs=\",\".join([str(i) for i in locations]),\n",
    "                sent=\" \".join(spellchk_sent)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n",
      "8\tThere was no doubt that Herr Schaffner meant every word of what he said .\n",
      "5,14\tJust before Myra left -- cathy was saying good-by to Cathy , and she did realize I was near '' .\n"
     ]
    }
   ],
   "source": [
    "print_corrected_sentence(\"4\\tit will put your maind into non-stop learning.\")\n",
    "print_corrected_sentence(\"8\\tThere was no doubt that Herr Schaffner meant evey word of what he said .\")\n",
    "print_corrected_sentence(\"5,14\\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, further progress can be seen however there seems to be an interference in the code that doesn't allow for the acceptance of apostrophe seperated words. In this case, it is reading it as didm and t thus resulting in the creation of the word did and not didn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object spellchk_v2 at 0x000001C204431300>\n"
     ]
    }
   ],
   "source": [
    "print(spellchk_v2(\"5,14\\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations of typos in the sentence: [5, 14]\n",
      "Corrected sentence: Just before Myra left -- she was saying good-by to Cathy, and she didm't realize I was near '' .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fh = \"5,14\\tJust before Myra left -- Sue was saying good-by to Cathy, and she didm't realize I was near '' .\"\n",
    "fh = StringIO(fh)  # Converts string to a file-like object\n",
    "\n",
    "for locations, spellchk_sent in spellchk_v2(fh):\n",
    "    print(f\"Locations of typos in the sentence: {locations}\")\n",
    "    print(f\"Corrected sentence: {' '.join(spellchk_sent)}\")\n",
    "    print(\"\\n\")  # Just for a clean separation between different sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is able to identify some of the potential errors within the sentence however it is not adequately handling the errors and fixing them in a correct manner. This much be due to its probability score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must further train the model to better understand the desired words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mind\n"
     ]
    }
   ],
   "source": [
    "print(select_correction_v2(\"maind\", fill_mask(\"it will put your [MASK] into non-stop learning.\", top_k=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can\n"
     ]
    }
   ],
   "source": [
    "print(select_correction_v2(\"cant'\", fill_mask(\"I [MASK] do this tonight.\", top_k=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: mind, Probability: 0.1139\n",
      "Word: talents, Probability: 0.0942\n",
      "Word: skills, Probability: 0.0432\n",
      "Word: brain, Probability: 0.0412\n",
      "Word: creativity, Probability: 0.0248\n",
      "Word: hands, Probability: 0.0232\n",
      "Word: imagination, Probability: 0.0197\n",
      "Word: brains, Probability: 0.0186\n",
      "Word: insight, Probability: 0.0155\n",
      "Word: lessons, Probability: 0.0138\n",
      "Word: life, Probability: 0.0127\n",
      "Word: skill, Probability: 0.0125\n",
      "Word: soul, Probability: 0.0115\n",
      "Word: fingers, Probability: 0.0114\n",
      "Word: students, Probability: 0.0108\n",
      "Word: talent, Probability: 0.0108\n",
      "Word: education, Probability: 0.0104\n",
      "Word: attention, Probability: 0.0099\n",
      "Word: learning, Probability: 0.0099\n",
      "Word: children, Probability: 0.0094\n"
     ]
    }
   ],
   "source": [
    "predictions = fill_mask(\"it will put your [MASK] into non-stop learning.\", top_k=20)\n",
    "for prediction in predictions:\n",
    "    word = prediction['token_str']\n",
    "    probability = prediction['score']\n",
    "    print(f\"Word: {word}, Probability: {probability:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the first mask:\n",
      "Word: cathy, Probability: 0.0883\n",
      "Word: she, Probability: 0.0323\n",
      "Word: myra, Probability: 0.0306\n",
      "Word: liz, Probability: 0.0300\n",
      "Word: janice, Probability: 0.0191\n",
      "Word: tammy, Probability: 0.0147\n",
      "Word: i, Probability: 0.0145\n",
      "Word: carol, Probability: 0.0123\n",
      "Word: mum, Probability: 0.0107\n",
      "Word: becky, Probability: 0.0102\n",
      "Word: laura, Probability: 0.0092\n",
      "Word: sara, Probability: 0.0090\n",
      "Word: josie, Probability: 0.0085\n",
      "Word: steve, Probability: 0.0080\n",
      "Word: melanie, Probability: 0.0078\n",
      "Word: susie, Probability: 0.0073\n",
      "Word: linda, Probability: 0.0067\n",
      "Word: sharon, Probability: 0.0067\n",
      "Word: beth, Probability: 0.0067\n",
      "Word: mom, Probability: 0.0065\n",
      "\n",
      "Predictions for the second mask:\n",
      "Word: did, Probability: 0.8966\n",
      "Word: would, Probability: 0.0343\n",
      "Word: could, Probability: 0.0338\n",
      "Word: might, Probability: 0.0170\n",
      "Word: must, Probability: 0.0069\n",
      "Word: should, Probability: 0.0046\n",
      "Word: does, Probability: 0.0021\n",
      "Word: had, Probability: 0.0007\n",
      "Word: may, Probability: 0.0006\n",
      "Word: didn, Probability: 0.0004\n",
      "Word: will, Probability: 0.0004\n",
      "Word: helped, Probability: 0.0002\n",
      "Word: suddenly, Probability: 0.0002\n",
      "Word: can, Probability: 0.0002\n",
      "Word: seemed, Probability: 0.0002\n",
      "Word: was, Probability: 0.0002\n",
      "Word: finally, Probability: 0.0001\n",
      "Word: soon, Probability: 0.0001\n",
      "Word: d, Probability: 0.0001\n",
      "Word: to, Probability: 0.0001\n",
      "\n",
      "Predictions for the third mask:\n",
      "Word: did, Probability: 0.8966\n",
      "Word: would, Probability: 0.0343\n",
      "Word: could, Probability: 0.0338\n",
      "Word: might, Probability: 0.0170\n",
      "Word: must, Probability: 0.0069\n",
      "Word: should, Probability: 0.0046\n",
      "Word: does, Probability: 0.0021\n",
      "Word: had, Probability: 0.0007\n",
      "Word: may, Probability: 0.0006\n",
      "Word: didn, Probability: 0.0004\n",
      "Word: will, Probability: 0.0004\n",
      "Word: helped, Probability: 0.0002\n",
      "Word: suddenly, Probability: 0.0002\n",
      "Word: can, Probability: 0.0002\n",
      "Word: seemed, Probability: 0.0002\n",
      "Word: was, Probability: 0.0002\n",
      "Word: finally, Probability: 0.0001\n",
      "Word: soon, Probability: 0.0001\n",
      "Word: d, Probability: 0.0001\n",
      "Word: to, Probability: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Predict for the first [MASK]\n",
    "predictions1 = fill_mask(\"Just before Myra left -- [MASK] was saying good-by to Cathy , and she didn't realize I was near\", top_k=20)\n",
    "print(\"Predictions for the first mask:\")\n",
    "for prediction in predictions1:\n",
    "    word = prediction['token_str']\n",
    "    probability = prediction['score']\n",
    "    print(f\"Word: {word}, Probability: {probability:.4f}\")\n",
    "\n",
    "# Predict for the second [MASK]\n",
    "predictions2 = fill_mask(\"Just before Myra left -- she was saying good-by to Cathy , and she [MASK] realize I was near\", top_k=20)\n",
    "print(\"\\nPredictions for the second mask:\")\n",
    "for prediction in predictions2:\n",
    "    word = prediction['token_str']\n",
    "    probability = prediction['score']\n",
    "    print(f\"Word: {word}, Probability: {probability:.4f}\")\n",
    "\n",
    "# Predict for the second [MASK]\n",
    "predictions3 = fill_mask(\"I am innocent, I [MASK] [MASK] commit the crime\", top_k=20)\n",
    "print(\"\\nPredictions for the third mask:\")\n",
    "for prediction in predictions2:\n",
    "    word = prediction['token_str']\n",
    "    probability = prediction['score']\n",
    "    print(f\"Word: {word}, Probability: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the code still seperates didm and t into seperate words and thus impacts the ability for it to select the correct replacement word in the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for the third mask:\n",
      "don\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# Predict for the two [MASK]\n",
    "predictions3 = fill_mask(\"I am innocent, I [MASK]'[MASK] commit the crime\", top_k=20)\n",
    "print(\"\\nPredictions for the third mask:\")\n",
    "for prediction in predictions3:\n",
    "    print(prediction[0]['token_str'])\n",
    "    # word = prediction['token_str']\n",
    "    # probability = prediction['score']\n",
    "    # print(f\"Word: {word}, Probability: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for the third mask:\n",
      "didn't\n"
     ]
    }
   ],
   "source": [
    "predictions4 = fill_mask(\"Just before Myra left -- she was saying good-by to Cathy , and she [MASK]'[MASK] realize I was near\", top_k=20)\n",
    "print(\"\\nPredictions for the third mask:\")\n",
    "    \n",
    "word = predictions4[0][0]['token_str'] + \"'\" + predictions4[1][0]['token_str']\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9831833293162868\n",
      "Cosine similarity: 0.9829060353262221\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def get_word_vector(word):\n",
    "    # Placeholder function - replace with actual model to get word vectors\n",
    "    # For example, using Word2Vec: return word2vec_model[word]\n",
    "    return np.random.rand(300)  # Dummy 300-dim vector\n",
    "\n",
    "def sentence_to_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [get_word_vector(word) for word in words]\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "def cosine_similarity(vecA, vecB):\n",
    "    return np.dot(vecA, vecB) / (norm(vecA) * norm(vecB))\n",
    "\n",
    "original_sentence = \"Just before Myra left -- Sue was saying good-by to Cathy, and she didn't realize I was near\"\n",
    "corrected_sentence_1 = \"Just before Myra left -- cathy was saying goodbye to Cathy, and she did realize I was near\"\n",
    "corrected_sentence_2 = \"Just before Myra left -- she was saying goodbye to Cathy, and she didn't realize I was near\"\n",
    "\n",
    "vec_original = sentence_to_vector(original_sentence)\n",
    "vec_corrected_1 = sentence_to_vector(corrected_sentence_1)\n",
    "vec_corrected_2 = sentence_to_vector(corrected_sentence_2)\n",
    "\n",
    "similarity_1 = cosine_similarity(vec_original, vec_corrected_1)\n",
    "similarity_2 = cosine_similarity(vec_original, vec_corrected_2)\n",
    "\n",
    "print(f\"Cosine similarity: {similarity_1}\")\n",
    "print(f\"Cosine similarity: {similarity_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert_score in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (2.2.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (2.2.0)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (4.37.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (1.26.3)\n",
      "Requirement already satisfied: requests in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (4.66.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from bert_score) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (2023.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from tqdm>=4.31.1->bert_score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from matplotlib->bert_score) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from matplotlib->bert_score) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from matplotlib->bert_score) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from matplotlib->bert_score) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from requests->bert_score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from requests->bert_score) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from requests->bert_score) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from requests->bert_score) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\trey2\\onedrive\\documents\\github\\thisgrouplovesnlp\\.venv\\lib\\site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: Precision: 0.9623104333877563, Recall: 0.9623662233352661, F1 Score: 0.9623383283615112\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score([corrected_sentence_1], [original_sentence], lang=\"en\")\n",
    "print(f\"BERTScore: Precision: {P.mean()}, Recall: {R.mean()}, F1 Score: {F1.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: Precision: 0.9794557094573975, Recall: 0.9716479182243347, F1 Score: 0.9755361676216125\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score([corrected_sentence_2], [original_sentence], lang=\"en\")\n",
    "print(f\"BERTScore: Precision: {P.mean()}, Recall: {R.mean()}, F1 Score: {F1.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
